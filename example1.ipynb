{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Title: Sentiment Analysis Example 1\n",
      "# Author: Dax Gerts\n",
      "# Date: 2 February 2016\n",
      "# Description: introductory example to sentiment analysis with python-nltk, based heavily on example provided by at www.nltk.org/howto/sentiment.html\n",
      "#   Example uses the 'sentiment anlyzer' tool to prepare data for classification with Naive Bayes Classifier\n",
      "\n",
      "## Modules\n",
      "\n",
      "# 1. NaiveBayesClassifier (machine learning method)\n",
      "\n",
      "# Naive Bayes Classifier is the machine learning model used in this example\n",
      "# More info https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "\n",
      "# 2. Subjectivity (the data)\n",
      "\n",
      "# Corpus of phrases divided between subjective/objective\n",
      "\n",
      "# \"The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences.\"\n",
      "# More info www.nltk.org/howto/corpus.html\n",
      "\n",
      "# For use in github notebook must manually download corpora\n",
      "nltk_dir = \"/home/nltk_data\"\n",
      "if nltk_dir not in nltk.data.path:\n",
      "        nltk.data.path.insert(0,nltk_dir)\n",
      "nltk.download(\"subjectivity\",download_dir=nltk_dir)\n",
      "#\n",
      "\n",
      "from nltk.corpus import subjectivity\n",
      "\n",
      "# 3. NLTK Sentiment Tools\n",
      "\n",
      "# The NLTK's collection of sentiment analysis tools\n",
      "# More info www.nltk.org\n",
      "\n",
      "from nltk.sentiment import SentimentAnalyzer\n",
      "from nltk.sentiment.util import *\n",
      "\n",
      "## Build training and testing data sets\n",
      "\n",
      "# Size of dataset(s)\n",
      "\n",
      "n = 1000\n",
      "\n",
      "# Get \"n\" subjective and objective phrases from subjectivity corpus\n",
      "\n",
      "subjective = [(sentences,'subj') for sentences in subjectivity.sents(categories='subj')[:n]]\n",
      "objective = [(sentences,'obj') for sentences in subjectivity.sents(categories='obj')[:n]]\n",
      "\n",
      "# Here's what the first item in \"subjective\" looks like\n",
      "# Note that it's stores as (phrase, label)\n",
      "\n",
      "subjective[0]\n",
      "\n",
      "# Create separate training and test data sets, this is pretty standard in any data mining/machine learning task\n",
      "# The typical split is, as seen here (training = 80%, train = 20%)\n",
      "\n",
      "training_subjective = subjective[:int(.8*n)]\n",
      "test_subjective = subjective[int(.8*n):n]\n",
      "training_objective = objective[:int(.8*n)]\n",
      "test_objective = objective[int(.8*n):n]\n",
      "\n",
      "# Now aggregate the training and test sets\n",
      "\n",
      "training = training_subjective + training_objective\n",
      "test = test_subjective + test_objective\n",
      "\n",
      "## Apply sentiment analysis to data to extract new \"features\"\n",
      "\n",
      "# Initialize sentiment analyzer object\n",
      "\n",
      "sentiment_analyzer = SentimentAnalyzer()\n",
      "\n",
      "# Mark all negative words in training data, using existing list of negative words\n",
      "\n",
      "all_negative_words = sentiment_analyzer.all_words([mark_negation(data) for data in training])\n",
      "\n",
      "unigram_features = sentiment_analyzer.unigram_word_feats(all_negative_words, min_freq=4)\n",
      "len(unigram_features)\n",
      "sentiment_analyzer.add_feat_extractor(extract_unigram_feats,unigrams=unigram_features)\n",
      "\n",
      "training_final = sentiment_analyzer.apply_features(training)\n",
      "test_final = sentiment_analyzer.apply_features(test)\n",
      "\n",
      "## Traing model and test\n",
      "\n",
      "model = NaiveBayesClassifier.train\n",
      "classifer = sentiment_analyzer.train(model, training_final)\n",
      "\n",
      "for key, value in sorted(sentiment_analyzer.evaluate(test_final).items()):\n",
      "    print('{0}: {1}'.format(key,value))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 13] Permission denied: '/home/nltk_data'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-065960bb109c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnltk_dir\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnltk_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subjectivity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    662\u001b[0m                                     subsequent_indent=prefix+prefix2+' '*4))\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36m_download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;31m# Ensure the download_dir exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mOSError\u001b[0m: [Errno 13] Permission denied: '/home/nltk_data'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package subjectivity to /home/nltk_data...\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}